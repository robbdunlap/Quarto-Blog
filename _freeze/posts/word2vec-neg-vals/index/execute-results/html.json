{
  "hash": "55c7b85122a0261186dc0bae7c52bcf2",
  "result": {
    "markdown": "---\ntitle: \"Understanding negative cosine similarity in Word2Vec generated vectors\"\nauthor: \"Robb Dunlap\"\ndate: \"2022-12-24\"\ncategories: [nlp,word2vec]\nimage: \"conny-schneider-xuTJZ7uD7PI-unsplash.jpg\"\n---\n\nThe key to understanding why cosine similarity of a W2V vector can be negative is to appreciate that the W2V vectors are not the same as vectors based on a simple token counting alorithm. If the vectorization system was based upon a simple count of the number of times a word showed up within a range of n-words in the training corpus then all cosine similarities for the vocabulary would range from 0 to 1. \n\n### Concrete example of a simple counting algorithm:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport re\nimport string\nimport pandas as pd\nimport numpy as np\n```\n:::\n\n\nStarting corpus:<br>\n\n> “The restaurant was fantastic and the waiters were experts. I recommend eating there if you want\nto experience the best of fine dining and excellent waiters.”\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ncorpus = \"\"\"\nThe restaurant was fantastic and the waiters were experts. I recommend eating there if you want\nto experience the best of fine dining and excellent waiters. \n\"\"\"\ncorpus = re.sub(r'\\n', ' ', corpus)\n\n# remove punctuation and starting/ending spaces and make all characters lowercase\ncorpus_no_punct = re.sub('[.,]', '', corpus)\ncorpus_no_punct = re.sub(r'^ ', '', corpus_no_punct)\ncorpus_no_punct = re.sub(r' $', '', corpus_no_punct)\ncorpus_no_punct_lc = corpus_no_punct.lower()\n```\n:::\n\n\nThe vocabulary is:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# generate the list of tokens\ncorpus_tokens = corpus_no_punct_lc.split()\ncorpus_tokens_set = set(corpus_tokens)\ncorpus_tokens_set\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n{'and',\n 'best',\n 'dining',\n 'eating',\n 'excellent',\n 'experience',\n 'experts',\n 'fantastic',\n 'fine',\n 'i',\n 'if',\n 'of',\n 'recommend',\n 'restaurant',\n 'the',\n 'there',\n 'to',\n 'waiters',\n 'want',\n 'was',\n 'were',\n 'you'}\n```\n:::\n:::\n\n\nAn example of a token window (-2 to +2) for the word eating is: <br>\n\n> “highly recommend **eating** there if” \n\n*\\*in this example the word window counting is done on the complete list of tokens from the input text, not on a sentence-by-sentence basis*\n\nThe vectors for \"the\", \"were\", and \"experts\" are (the vectors are the columns):\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ninput_text_zeroes_dict = {k:0 for k in set(corpus_tokens)}\n\ninput_text_counted_dict = input_text_zeroes_dict.copy()\n\nfor word in corpus_tokens:\n    input_text_counted_dict[word] += 1\n\nto_pandas_dict = {k:[v] for k,v in input_text_counted_dict.items()}\nresults_df = pd.DataFrame.from_dict(to_pandas_dict, orient='index', columns=['vocab_count'])\n\ndef position_checker(i,j, len_lst):\n    if i+j < 0:\n        return False\n    elif i+j >= len_lst:\n        return False\n    else:\n        return True\n\n# add each unique word as a column\nfor word in set(corpus_tokens):\n    results_df[word] = 0\n\nlen_corpus_tokens = len(corpus_tokens)\n\n# loop through every word in the list\nfor i,word in enumerate(corpus_tokens):\n    # loop through the 4 surrounding words start at -2 through +2\n\n    # dict to hold each word vector\n    word_vects_dict = input_text_zeroes_dict.copy()\n\n    for j in [-2,-1,1,2]:\n        # use a try block, that way the first two words at the beginning of the list and at the end won't fail\n        check = position_checker(i,j, len_corpus_tokens)\n        if check:\n            results_df.loc[corpus_tokens[i + j], word] += 1\n\nchoices = ['the','were','experts']\nresults_df[choices].sort_index()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>the</th>\n      <th>were</th>\n      <th>experts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>and</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>best</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>dining</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>eating</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>excellent</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>experience</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>experts</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>fantastic</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>fine</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>i</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>if</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>recommend</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>restaurant</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>the</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>there</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>to</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>waiters</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>want</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>was</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>were</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>you</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe scalars within each vector range from 0 to m (the count of the word that had the highest count in the corpus). When comparing two vectors using this system then the cosine similarity would always scale from 0 (orthogonal) to 1 (aligned). For example, cosine similarity for the above three words is:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef cosine_similarity(A, B):\n    dot = np.dot(A,B)    \n    norma = np.linalg.norm(A)\n    normb = np.linalg.norm(B)\n    cos = dot / (norma * normb)\n    return cos\n\ncossim0_1 = cosine_similarity(results_df[choices[0]], results_df[choices[1]])\ncossim0_2 = cosine_similarity(results_df[choices[0]], results_df[choices[2]])\ncossim1_2 = cosine_similarity(results_df[choices[1]], results_df[choices[2]])\n\nprint(f'\"{choices[0]}\" and \"{choices[1]}\":      {cossim0_1:.4f}')\nprint(f'\"{choices[0]}\" and \"{choices[2]}\":   {cossim0_2:.4f}')\nprint(f'\"{choices[1]}\" and \"{choices[2]}\":  {cossim1_2:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\"the\" and \"were\":      0.1581\n\"the\" and \"experts\":   0.3162\n\"were\" and \"experts\":  0.5000\n```\n:::\n:::\n\n\n### Why W2V Can Have Negative Cosine Similarities\n\nW2V works differently than the simple counting example above. W2V is a 2-layer neural net (a hidden layer and an output layer). The model was trained to predict a missing word in a sentence (CBOW) and also the surrounding context words of a word in a sentence (Skipgram). While it was trained to do this function, the crafty part of the W2V is that we don’t care about the output after it has been trained. Instead, we want the embedding which is the value of the hidden layer for a word of interest (if you are unfamiliar with the inner workings of W2V then I recommend reading Chris McCormick’s blog on it - http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). The net result is that the scalar values within the embedding vector can be -1 to +1. As such, it’s possible to have one vector diametrically opposed to another and thus a cosine similarity of -1, something that is not possible with the simple token window frequency approach. \n\nMy interpretation of a negative cosine similarity value for two words is that they are unrelated or possibly of opposite value but not necessarily antonyms of each other. I can think of examples where true antonyms might have high cosine similarity scores because, as John Rupert Firth might have put it, they keep the same company (https://en.wikipedia.org/wiki/John_Rupert_Firth). \n\n\n\n*Photo by <a href=\"https://unsplash.com/@choys_?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Conny Schneider</a> on <a href=\"https://unsplash.com/photos/xuTJZ7uD7PI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>*\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}