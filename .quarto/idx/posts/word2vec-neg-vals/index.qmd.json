{"title":"Understanding negative cosine similarity in Word2Vec generated vectors","markdown":{"yaml":{"title":"Understanding negative cosine similarity in Word2Vec generated vectors","author":"Robb Dunlap","date":"2022-12-24","categories":["nlp","word2vec"],"image":"conny-schneider-xuTJZ7uD7PI-unsplash.jpg"},"headingText":"Concrete example of a simple counting algorithm:","containsRefs":false,"markdown":"\n\nThe key to understanding why cosine similarity of a W2V vector can be negative is to appreciate that the W2V vectors are not the same as vectors based on a simple token counting alorithm. If the vectorization system was based upon a simple count of the number of times a word showed up within a range of n-words in the training corpus then all cosine similarities for the vocabulary would range from 0 to 1. \n\n\n```{python}\nimport re\nimport string\nimport pandas as pd\nimport numpy as np\n```\n\nStarting corpus:<br>\n\n> “The restaurant was fantastic and the waiters were experts. I recommend eating there if you want\nto experience the best of fine dining and excellent waiters.”\n\n```{python}\ncorpus = \"\"\"\nThe restaurant was fantastic and the waiters were experts. I recommend eating there if you want\nto experience the best of fine dining and excellent waiters. \n\"\"\"\ncorpus = re.sub(r'\\n', ' ', corpus)\n\n# remove punctuation and starting/ending spaces and make all characters lowercase\ncorpus_no_punct = re.sub('[.,]', '', corpus)\ncorpus_no_punct = re.sub(r'^ ', '', corpus_no_punct)\ncorpus_no_punct = re.sub(r' $', '', corpus_no_punct)\ncorpus_no_punct_lc = corpus_no_punct.lower()\n```\n\nThe vocabulary is:\n\n```{python}\n# generate the list of tokens\ncorpus_tokens = corpus_no_punct_lc.split()\ncorpus_tokens_set = set(corpus_tokens)\ncorpus_tokens_set\n```\n\n\nAn example of a token window (-2 to +2) for the word eating is: <br>\n\n> “highly recommend **eating** there if” \n\n*\\*in this example the word window counting is done on the complete list of tokens from the input text, not on a sentence-by-sentence basis*\n\nThe vectors for \"the\", \"were\", and \"experts\" are (the vectors are the columns):\n\n```{python}\ninput_text_zeroes_dict = {k:0 for k in set(corpus_tokens)}\n\ninput_text_counted_dict = input_text_zeroes_dict.copy()\n\nfor word in corpus_tokens:\n    input_text_counted_dict[word] += 1\n\nto_pandas_dict = {k:[v] for k,v in input_text_counted_dict.items()}\nresults_df = pd.DataFrame.from_dict(to_pandas_dict, orient='index', columns=['vocab_count'])\n\ndef position_checker(i,j, len_lst):\n    if i+j < 0:\n        return False\n    elif i+j >= len_lst:\n        return False\n    else:\n        return True\n\n# add each unique word as a column\nfor word in set(corpus_tokens):\n    results_df[word] = 0\n\nlen_corpus_tokens = len(corpus_tokens)\n\n# loop through every word in the list\nfor i,word in enumerate(corpus_tokens):\n    # loop through the 4 surrounding words start at -2 through +2\n\n    # dict to hold each word vector\n    word_vects_dict = input_text_zeroes_dict.copy()\n\n    for j in [-2,-1,1,2]:\n        # use a try block, that way the first two words at the beginning of the list and at the end won't fail\n        check = position_checker(i,j, len_corpus_tokens)\n        if check:\n            results_df.loc[corpus_tokens[i + j], word] += 1\n\nchoices = ['the','were','experts']\nresults_df[choices].sort_index()\n```\n\n\nThe scalars within each vector range from 0 to m (the count of the word that had the highest count in the corpus). When comparing two vectors using this system then the cosine similarity would always scale from 0 (orthogonal) to 1 (aligned). For example, cosine similarity for the above three words is:\n\n```{python}\ndef cosine_similarity(A, B):\n    dot = np.dot(A,B)    \n    norma = np.linalg.norm(A)\n    normb = np.linalg.norm(B)\n    cos = dot / (norma * normb)\n    return cos\n\ncossim0_1 = cosine_similarity(results_df[choices[0]], results_df[choices[1]])\ncossim0_2 = cosine_similarity(results_df[choices[0]], results_df[choices[2]])\ncossim1_2 = cosine_similarity(results_df[choices[1]], results_df[choices[2]])\n\nprint(f'\"{choices[0]}\" and \"{choices[1]}\":      {cossim0_1:.4f}')\nprint(f'\"{choices[0]}\" and \"{choices[2]}\":   {cossim0_2:.4f}')\nprint(f'\"{choices[1]}\" and \"{choices[2]}\":  {cossim1_2:.4f}')\n```\n\n\n### Why W2V Can Have Negative Cosine Similarities\n\nW2V works differently than the simple counting example above. W2V is a 2-layer neural net (a hidden layer and an output layer). The model was trained to predict a missing word in a sentence (CBOW) and also the surrounding context words of a word in a sentence (Skipgram). While it was trained to do this function, the crafty part of the W2V is that we don’t care about the output after it has been trained. Instead, we want the embedding which is the value of the hidden layer for a word of interest (if you are unfamiliar with the inner workings of W2V then I recommend reading Chris McCormick’s blog on it - http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). The net result is that the scalar values within the embedding vector can be -1 to +1. As such, it’s possible to have one vector diametrically opposed to another and thus a cosine similarity of -1, something that is not possible with the simple token window frequency approach. \n\nMy interpretation of a negative cosine similarity value for two words is that they are unrelated or possibly of opposite value but not necessarily antonyms of each other. I can think of examples where true antonyms might have high cosine similarity scores because, as John Rupert Firth might have put it, they keep the same company (https://en.wikipedia.org/wiki/John_Rupert_Firth). \n\n\n\n*Photo by <a href=\"https://unsplash.com/@choys_?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Conny Schneider</a> on <a href=\"https://unsplash.com/photos/xuTJZ7uD7PI?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>*"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","theme":"cosmo","title-block-banner":true,"title":"Understanding negative cosine similarity in Word2Vec generated vectors","author":"Robb Dunlap","date":"2022-12-24","categories":["nlp","word2vec"],"image":"conny-schneider-xuTJZ7uD7PI-unsplash.jpg"},"extensions":{"book":{"multiFile":true}}}}}