[
  {
    "objectID": "posts/python-cheat-sheet/index.html",
    "href": "posts/python-cheat-sheet/index.html",
    "title": "Cheat Sheet for Simple Things in Python",
    "section": "",
    "text": "Format numbers with commas in a print statement\nnum_to_display = 1000000\nprint(f'displaying the number with commas {num_to_display:,}')\n\n>>> 1,000,000\n\n\n\nFormat floats as percentage in a print statement\nnum_to_display = 10/22\nprint(f'displaying the number with commas {num_to_display:.2%}')\n\n>>> 45.45%\n\n\n\nFormat floats with fixed number of decimals in a print statement\nnum_to_display = 10/22\nprint(f'displaying the number with commas {num_to_display:.2f}')\n\n>>> 0.45\n #### Use regex in Pandas - no sense in duplicating the excellent material on this blog, just go see the original source material\n Photo by Clément Hélardot on Unsplash"
  },
  {
    "objectID": "posts/word2vec-neg-vals/index.html",
    "href": "posts/word2vec-neg-vals/index.html",
    "title": "Understanding negative cosine similarity in Word2Vec generated vectors",
    "section": "",
    "text": "The key to understanding why cosine similarity of a W2V vector can be negative is to appreciate that the W2V vectors are not the same as vectors based on a simple token counting alorithm. If the vectorization system was based upon a simple count of the number of times a word showed up within a range of n-words in the training corpus then all cosine similarities for the vocabulary would range from 0 to 1.\n\nConcrete example of a simple counting algorithm:\n\nimport re\nimport string\nimport pandas as pd\nimport numpy as np\n\nStarting corpus:\n\n“The restaurant was fantastic and the waiters were experts. I recommend eating there if you want to experience the best of fine dining and excellent waiters.”\n\n\ncorpus = \"\"\"\nThe restaurant was fantastic and the waiters were experts. I recommend eating there if you want\nto experience the best of fine dining and excellent waiters. \n\"\"\"\ncorpus = re.sub(r'\\n', ' ', corpus)\n\n# remove punctuation and starting/ending spaces and make all characters lowercase\ncorpus_no_punct = re.sub('[.,]', '', corpus)\ncorpus_no_punct = re.sub(r'^ ', '', corpus_no_punct)\ncorpus_no_punct = re.sub(r' $', '', corpus_no_punct)\ncorpus_no_punct_lc = corpus_no_punct.lower()\n\nThe vocabulary is:\n\n# generate the list of tokens\ncorpus_tokens = corpus_no_punct_lc.split()\ncorpus_tokens_set = set(corpus_tokens)\ncorpus_tokens_set\n\n{'and',\n 'best',\n 'dining',\n 'eating',\n 'excellent',\n 'experience',\n 'experts',\n 'fantastic',\n 'fine',\n 'i',\n 'if',\n 'of',\n 'recommend',\n 'restaurant',\n 'the',\n 'there',\n 'to',\n 'waiters',\n 'want',\n 'was',\n 'were',\n 'you'}\n\n\nAn example of a token window (-2 to +2) for the word eating is: \n\n“highly recommend eating there if”\n\n*in this example the word window counting is done on the complete list of tokens from the input text, not on a sentence-by-sentence basis\nThe vectors for “the”, “were”, and “experts” are (the vectors are the columns):\n\ninput_text_zeroes_dict = {k:0 for k in set(corpus_tokens)}\n\ninput_text_counted_dict = input_text_zeroes_dict.copy()\n\nfor word in corpus_tokens:\n    input_text_counted_dict[word] += 1\n\nto_pandas_dict = {k:[v] for k,v in input_text_counted_dict.items()}\nresults_df = pd.DataFrame.from_dict(to_pandas_dict, orient='index', columns=['vocab_count'])\n\ndef position_checker(i,j, len_lst):\n    if i+j < 0:\n        return False\n    elif i+j >= len_lst:\n        return False\n    else:\n        return True\n\n# add each unique word as a column\nfor word in set(corpus_tokens):\n    results_df[word] = 0\n\nlen_corpus_tokens = len(corpus_tokens)\n\n# loop through every word in the list\nfor i,word in enumerate(corpus_tokens):\n    # loop through the 4 surrounding words start at -2 through +2\n\n    # dict to hold each word vector\n    word_vects_dict = input_text_zeroes_dict.copy()\n\n    for j in [-2,-1,1,2]:\n        # use a try block, that way the first two words at the beginning of the list and at the end won't fail\n        check = position_checker(i,j, len_corpus_tokens)\n        if check:\n            results_df.loc[corpus_tokens[i + j], word] += 1\n\nchoices = ['the','were','experts']\nresults_df[choices].sort_index()\n\n\n\n\n\n  \n    \n      \n      the\n      were\n      experts\n    \n  \n  \n    \n      and\n      1\n      0\n      0\n    \n    \n      best\n      1\n      0\n      0\n    \n    \n      dining\n      0\n      0\n      0\n    \n    \n      eating\n      0\n      0\n      0\n    \n    \n      excellent\n      0\n      0\n      0\n    \n    \n      experience\n      1\n      0\n      0\n    \n    \n      experts\n      0\n      1\n      0\n    \n    \n      fantastic\n      1\n      0\n      0\n    \n    \n      fine\n      0\n      0\n      0\n    \n    \n      i\n      0\n      1\n      1\n    \n    \n      if\n      0\n      0\n      0\n    \n    \n      of\n      1\n      0\n      0\n    \n    \n      recommend\n      0\n      0\n      1\n    \n    \n      restaurant\n      1\n      0\n      0\n    \n    \n      the\n      0\n      1\n      0\n    \n    \n      there\n      0\n      0\n      0\n    \n    \n      to\n      1\n      0\n      0\n    \n    \n      waiters\n      1\n      1\n      1\n    \n    \n      want\n      0\n      0\n      0\n    \n    \n      was\n      1\n      0\n      0\n    \n    \n      were\n      1\n      0\n      1\n    \n    \n      you\n      0\n      0\n      0\n    \n  \n\n\n\n\nThe scalars within each vector range from 0 to m (the count of the word that had the highest count in the corpus). When comparing two vectors using this system then the cosine similarity would always scale from 0 (orthogonal) to 1 (aligned). For example, cosine similarity for the above three words is:\n\ndef cosine_similarity(A, B):\n    dot = np.dot(A,B)    \n    norma = np.linalg.norm(A)\n    normb = np.linalg.norm(B)\n    cos = dot / (norma * normb)\n    return cos\n\ncossim0_1 = cosine_similarity(results_df[choices[0]], results_df[choices[1]])\ncossim0_2 = cosine_similarity(results_df[choices[0]], results_df[choices[2]])\ncossim1_2 = cosine_similarity(results_df[choices[1]], results_df[choices[2]])\n\nprint(f'\"{choices[0]}\" and \"{choices[1]}\":      {cossim0_1:.4f}')\nprint(f'\"{choices[0]}\" and \"{choices[2]}\":   {cossim0_2:.4f}')\nprint(f'\"{choices[1]}\" and \"{choices[2]}\":  {cossim1_2:.4f}')\n\n\"the\" and \"were\":      0.1581\n\"the\" and \"experts\":   0.3162\n\"were\" and \"experts\":  0.5000\n\n\n\n\nWhy W2V Can Have Negative Cosine Similarities\nW2V works differently than the simple counting example above. W2V is a 2-layer neural net (a hidden layer and an output layer). The model was trained to predict a missing word in a sentence (CBOW) and also the surrounding context words of a word in a sentence (Skipgram). While it was trained to do this function, the crafty part of the W2V is that we don’t care about the output after it has been trained. Instead, we want the embedding which is the value of the hidden layer for a word of interest (if you are unfamiliar with the inner workings of W2V then I recommend reading Chris McCormick’s blog on it - http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). The net result is that the scalar values within the embedding vector can be -1 to +1. As such, it’s possible to have one vector diametrically opposed to another and thus a cosine similarity of -1, something that is not possible with the simple token window frequency approach.\nMy interpretation of a negative cosine similarity value for two words is that they are unrelated or possibly of opposite value but not necessarily antonyms of each other. I can think of examples where true antonyms might have high cosine similarity scores because, as John Rupert Firth might have put it, they keep the same company (https://en.wikipedia.org/wiki/John_Rupert_Firth).\nPhoto by Conny Schneider on Unsplash"
  },
  {
    "objectID": "posts/useful-sites-links/index.html",
    "href": "posts/useful-sites-links/index.html",
    "title": "Links to Useful Programming References",
    "section": "",
    "text": "Markdown Cheatsheet\n\n\nPandoc Cheatsheet\n\nPhoto by Bilal Karim on Unsplash"
  },
  {
    "objectID": "posts/quarto-operations/index.html",
    "href": "posts/quarto-operations/index.html",
    "title": "Checklist for Publishing on Quarto",
    "section": "",
    "text": "To create a new post\n\nCreate a new folder in the ./posts/ folder - give it a name relevant to the post\nPut a copy of a relevant image into the folder\nCreate a new file in the just created folder - name it “index.qmd”\nAdd the TOML header to the file\n\nChange the image file name to match the image in the folder\n\nAdd the photo attribution to the bottom of the file with two <br> elements separating them from the text above and italisize the attribution\nWrite the post in the body of the file\nView the post in draft using Render\nPublish the post to website using the command line\n\nnavigate to the overall blog folder\nuse the command “quarto publish quarto-pub”\nselect the existing project\n\n\n\n\n\nUseful References\n\nQuarto: Creating a Blog\nQuarto: Quarto Publishing\n\n\nPhoto by Andrés Dallimonti on Unsplash"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Cheat Sheet for Simple Things in Python\n\n\n\n\n\n\n\npython\n\n\ncheatsheet\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2022\n\n\nRobb Dunlap\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding negative cosine similarity in Word2Vec generated vectors\n\n\n\n\n\n\n\nnlp\n\n\nword2vec\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2022\n\n\nRobb Dunlap\n\n\n\n\n\n\n  \n\n\n\n\nLinks to Useful Programming References\n\n\n\n\n\n\n\nreferences\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2022\n\n\nRobb Dunlap\n\n\n\n\n\n\n  \n\n\n\n\nChecklist for Publishing on Quarto\n\n\n\n\n\n\n\nquarto\n\n\nsop\n\n\nreferences\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2022\n\n\nRobb Dunlap\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Just a collection of my thoughts or notes on Python programming and natural language processing"
  }
]